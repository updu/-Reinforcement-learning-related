import numpy as np
import random
import cv2
from copy import deepcopy
import matplotlib.pyplot as plt

N = 0
E = 1
S = 2
W = 3
exit = 1
dir = ['N', 'E', 'S', 'W']


class RLMaze():
    """
    """

    def __init__(self, height=None, width=None, goal=None, nogoal=None, start=None, obstacles=None):

        # initialise cordinates and maze parameters

        self.height = height or 3
        self.width = width or 4
        self.goal = goal or [0, 3]
        self.nogoal = nogoal or [1, 3]
        self.obstacles = obstacles or [[1, 1]]
        self.start = start or [2, 0]
        self.pos = self.start
        self.maze = self.generate_maze()
        self.reward = self.maze
        self.prev_pos = None
        self.actions = [0, 1, 2, 3]
        self.neighbours = [[-1, 0], [0, 1], [1, 0], [0, -1]]
        # self.move=np.array([[0,0,1],[1,0,0],[0,1,1],[1,1,0],[1,1,2],[2,1,1],[1,2,2],[2,2,1],[2,2,3],[3,2,2],[2,3,3],[3,3,2],[3,3,0],[0,3,3],[3,0,0],[0,0,3]])

        # learning parameters
        self.movement_cost = -0.04
        self.alpha = 0.8
        self.gamma = 0.9
        self.epsilon = 0.1


        # learning variables
        self.policy = ""
        self.maxpolicy = ""
        self.total_reward = -100
        self.iteration = 1
        self.best = [""]
        self.epoch_num = 0
        self.cycle_num = 0
        self.move_reward = 0
        self.exit = 1
        self.reached = np.zeros([self.height, self.width])
        self.Q = np.zeros((self.height, self.width, len(self.actions)), dtype=np.float32)

        # image parameters
        self.pixelpercell = 50
        self.pause = 1
        self.img = np.zeros([self.pixelpercell * self.maze.shape[0], self.pixelpercell * self.maze.shape[1], 3],
                            np.uint8)
        self.img = self.generate_background()

        #目标
        self.target = 0
        self.ax = []
        self.ay = []
        self.i_count = 1
        self.i = 0

        self.step_num_x = []
        self.step_num_y = []

        self.reward_x = []
        self.reward_y = []



    def generate_maze(self):
        maze = np.zeros([self.height, self.width])      #迷宫：高 x 宽
        maze[self.goal[0], self.goal[1]] = 1            #目标点为1
        #maze[self.nogoal[0], self.nogoal[1]] = -1       #陷阱为-1
        for nog in self.nogoal:
            maze[nog[0], nog[1]] = -1
        for ob in self.obstacles:
            maze[ob[0], ob[1]] = -10            #障碍物为-10
        return maze

    def generate_background(self):
        self.img[:, :, :] = 255
        c = self.img.shape[0]

        for i in range(1, self.maze.shape[0]):
            a = int(c * i / self.maze.shape[0])
            cv2.line(self.img, (0, a), (self.img.shape[1], a), [0, 0, 0], 2)
        c = self.img.shape[1]
        for i in range(1, self.maze.shape[1]):
            a = int(c * i / self.maze.shape[1])
            cv2.line(self.img, (a, 0), (a, self.img.shape[0]), [0, 0, 0], 2)
        for i in range(self.maze.shape[0]):
            for j in range(self.maze.shape[1]):
                spl = False
                if [i, j] == self.goal:
                    color = [0, 255, 0]
                    spl = True
                elif [i, j] in self.nogoal:
                    color = [0, 0, 255]
                    spl = True
                if [i, j] in self.obstacles:
                    color = [255, 0, 0]
                    spl = True
                if spl:
                    a = self.pixelpercell * i
                    c = self.pixelpercell * (i + 1)
                    b = self.pixelpercell * j
                    d = self.pixelpercell * (j + 1)
                    cv2.rectangle(self.img, (b, a), (d, c), color, thickness=-1)
                    #print(a,b,c,d)

        cv2.imshow('Environment', self.img)
        return self.img

    def getQ(self, state, action):
        if state == self.goal or state == self.nogoal:
            return self.Q[state[0], state[1], 0]        #假如所在状态为目标或者陷阱，则返回所在状态的第一个值
        else:
            return self.Q[state[0], state[1], action]   #否则返回所在状态所采取的动作的值

    def putQ(self, state, action, q):
        if state == self.goal or state == self.nogoal:
            self.Q[state[0], state[1], 0] = q           #假如所在状态为目标或者陷阱，则将q值赋予所在状态的第一个值
        else:
            self.Q[state[0], state[1], action] = q      #否则将q值赋予所在状态所采取的动作的值

    def action(self, state):                            #动作选取
        self.epsilon = 1/(np.sqrt(self.epoch_num) + 1)
        if random.random() < self.epsilon:
            action = random.choice(self.actions)        #假如随机数小于epsilon，则随机选取一个动作

        else:
            q = [self.getQ(state, a) for a in self.actions]     #获取所在状态四个动作的q值
            maxQ = max(q)                               #求四个q值中最大的值
            count = q.count(maxQ)                       #计算机四个q值中有几个值和最大值相同
            if count > 1:
                best = [i for i in range(len(self.actions)) if q[i] == maxQ]
                i = random.choice(best)                 #从相同值操作中随机选择一个
            else:
                i = q.index(maxQ)                       #否则选取值最大的操作

            action = self.actions[i]

        return action

    def action1(self, state):                            #动作选取

        q = [self.getQ(state, a) for a in self.actions]     #获取所在状态四个动作的q值
        maxQ = max(q)                               #求四个q值中最大的值
        count = q.count(maxQ)                       #计算机四个q值中有几个值和最大值相同
        if count > 1:
            best = [i for i in range(len(self.actions)) if q[i] == maxQ]
            i = random.choice(best)                 #从相同值操作中随机选择一个
        else:
            i = q.index(maxQ)                       #否则选取值最大的操作

        action = self.actions[i]

        return action



    def render(self):
        self.env = self.generate_background()
        a = self.pixelpercell * self.pos[0]
        c = self.pixelpercell * (self.pos[0] + 1)
        b = self.pixelpercell * self.pos[1]
        d = self.pixelpercell * (self.pos[1] + 1)
        cv2.rectangle(self.env, (b + 10, a + 10), (d - 10, c - 10), [0, 255, 255], thickness=-1)
        for i in range(self.maze.shape[0]):
            for j in range(self.maze.shape[1]):
                c = self.pixelpercell * (i + 0.54)
                d = self.pixelpercell * (j + 0.28)
                for ac in self.actions:
                    a = c + self.pixelpercell * 0.225 * self.neighbours[ac][0]
                    b = d + self.pixelpercell * 0.225 * self.neighbours[ac][1]
                    cv2.putText(self.env, str(int(10000 * self.Q[i][j][ac]) / 100.0)[0:-1], (int(b), int(a)),
                                cv2.FONT_HERSHEY_PLAIN, 0.6, (0, 0, 0), 1)

        cv2.imshow('Environment', self.env)
        #cv2.imwrite("CustomEnv/{}E{}.jpg".format(self.epoch_num,self.cycle_num),self.env)
        cv2.waitKey(self.pause)

    def MoveDir(self, state, action):
        h = state[0] + self.neighbours[action][0]                                   #根据所选动作来改变横坐标
        w = state[1] + self.neighbours[action][1]                                   #根据所选动作来改变纵坐标
        if ((-1 < h < self.height) and (-1 < w < self.width)):
            if (self.maze[h][w] != -10):
                state[0] = state[0] + self.neighbours[action][0]                    #下一个动作的横坐标
                state[1] = state[1] + self.neighbours[action][1]                    #下一个动作的纵坐标

        if (state == self.goal): #or (state == self.nogoal):                        #假如下一个状态时目标或者陷阱
            exit = 0                                                                #exit = 0
            print("目标")                                                           #打印“exit”
            self.target = 1
            self.i += 1
            self.ax.append(self.i)
            self.step_num_x.append(self.i)
            self.reward_x.append(self.i)

        elif (state in self.nogoal):
            exit = 0
            print("陷阱")
            self.i += 0
            self.ax.append(self.i)
        else:
            exit = 1                                                                #其他情况 exit = 10



        move_reward = (self.reward[state[0]][state[1]] + self.movement_cost)        #移动反馈 = 下一个状态的reward值 + 移动损失值
        return exit, state, move_reward

    def Movec(self, state, act):
        c = ""
        exit, state, move_reward = self.MoveDir(state, act)         #返回exit，状态，移动反馈
        c += str(dir[act])
        self.policy += c
        self.policy += '|'
        self.reached[state[0]][state[1]] += 1
        return exit, state, move_reward

    def learn(self):
        maxr = self.total_reward
        while (self.epoch_num < 100):

            self.Epoch()
            self.render()

            self.epoch_num += 1

            #画图

            self.ay.append(self.i_count)
            self.i_count += 1

            print(maxr)

            # if ((self.total_reward) >= maxr):
            if (self.target == 1):
                maxr = self.total_reward

                # print("Epoch {} with max rewdrd={} and policy = {}".format(self.epoch_num, maxr,self.policy))
                print("Epoch {} and policy = {} and cycle {}".format(self.epoch_num, self.policy, self.cycle_num))
                self.step_num_y.append(self.cycle_num)
                self.target = 0

                self.reward_y.append(self.total_reward)


    def Epoch(self):

        self.reset()
        self.exit = 1
        n = 0
        self.policy = ""
        self.cycle_num = 0

        while (self.exit == 1):
            self.prev_pos = deepcopy(self.pos)
            n += 1
            act = self.action1(self.pos)                                             #在当前状态选取动作
            Q = self.getQ([self.prev_pos[0], self.prev_pos[1]], act)                #返回在该状态下采取动作后的Q值
            self.exit, pos, self.move_reward = self.Movec(self.pos, act)            #exit:下一个状态是目标或是陷阱或是障碍或是其他，pos：返回采取动作后的下一个状态，move_reward = 下一个状态的reward值 + 移动损失值
            self.total_reward += self.move_reward                                   #total_reward = total_reward + move_reward
            self.render()
            act1 = self.action(pos)
            maxQ = self.getQ(pos, act1)
            # q = [self.getQ(pos, a) for a in self.actions]
            # maxQ = max(q)

            Q = Q + self.alpha * (self.move_reward + (self.gamma * maxQ) - Q)
            self.putQ([self.prev_pos[0], self.prev_pos[1]], act, Q)
            self.pos = pos
            self.cycle_num += 1

        return exit

    def reset(self):
        self.total_reward = 0
        self.pos = [self.start[0], self.start[1]]
        self.render()
        print("reset")
        self.reached = np.zeros([self.height, self.width])
        self.exit = 1

    def draw(self):
        plt.clf()
        plt.plot(self.ay, self.ax)
        plt.xlabel = ('训练的总次数')
        plt.ylabel = ('成功到达终点的总次数')
        plt.pause(5)
        plt.savefig('成功次数')
        #plt.ioff()

    def draw_1(self):
        plt.clf()
        plt.xlabel = ('成功达到终点的次数')
        plt.ylabel = ('每次达到终点走的步数')
        plt.plot(self.step_num_x, self.step_num_y)
        plt.pause(5)
        plt.savefig('成功步数')

    def draw_2(self):
        plt.clf()
        plt.xlabel = ('成功达到终点的次数')
        plt.ylabel = ('每次成功到达终点的总奖励值')
        plt.plot(self.reward_x, self.reward_y)
        plt.pause(5)
        plt.savefig('reward')
        np.savetxt('100_sarsa_reward_x.csv',self.reward_x,delimiter=',')
        np.savetxt('100_sarsa_reward_y.csv', self.reward_y,delimiter=',')


    def out(self):
        print("ax:",self.ax,"ay:",self.ay)


rl = RLMaze(height=20, width=20,
            goal=[18, 18],
            nogoal=[[18, 2], [2, 18], [13, 15], [18, 13]],
            start=[0, 0],
            obstacles=[[0, 4], [1, 4], [2, 4], [3, 4], [4, 4], [8, 1], [8, 2], [8, 3], [8, 4],
                       [12, 4], [13, 4], [14, 4], [15, 4],
                       [4, 8], [5, 8], [6, 8], [7, 8], [8, 8], [9, 8],
                       [13, 8], [14, 8], [15, 8], [16, 8], [17, 8],
                       [4, 11], [4, 12], [4, 13], [3, 13], [2, 13],
                       [8, 12], [9, 12], [10, 12], [11, 12],
                       [15, 11], [15, 12], [15, 13], [15, 14], [15, 15],
                       [3, 16], [4, 16], [5, 16], [6, 16], [7 ,16], [8, 16],
                       [13, 18], [13, 19]])
rl.learn()
rl.out()
rl.draw()
rl.draw_1()
rl.draw_2()
